{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Task - DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file covers the code for the joint advanced tasks of the coursework. We define the same custom grid world enviroment as the basic task, with a maze to be solved in as few steps as possible, but with a few modifications. We implement DQN and two improvements: double and dueling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \n",
    "    def __init__(self, N=25):\n",
    "        \"\"\"\n",
    "        This class defines a maze environment in a square 2-dimensional numpy array.\n",
    "        N: dimensions of array, integer, default 25\n",
    "\n",
    "        There are penalties placed at every dead end in the maze.\n",
    "        The agent and door are at randomised positions in the maze.\n",
    "        In the array, these are represented by:\n",
    "        0: part of the maze path, 1: the agent, 2: maze wall/obstacle, 4: door\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "\n",
    "        # Generating the maze.\n",
    "        self.grid = np.full((N,N),2)\n",
    "        self.generate_maze()\n",
    "\n",
    "        # Placing penalties at every dead end.\n",
    "        self.penalties_pos = self.generate_penalties()\n",
    "\n",
    "        for cell in self.penalties_pos:\n",
    "            self.grid[cell] = 3\n",
    "\n",
    "        # Adding door and agent at random positions, and getting the current state of the agent.\n",
    "        self.door_pos = self.get_rand_empty_cells(1)\n",
    "        self.grid[self.door_pos] = 4\n",
    "\n",
    "        self.agent_pos = self.get_rand_empty_cells(1)\n",
    "        self.grid[self.agent_pos] = 1\n",
    "\n",
    "        self.state = self.get_state(self.agent_pos)\n",
    "\n",
    "        # Setting a step counter, and the maximum number of steps per episode, which is N squared.\n",
    "        self.time_elapsed = 0\n",
    "        self.time_limit = self.N*self.N\n",
    "\n",
    "        # Defining actions available to the agent.\n",
    "        self.actions = [('up',-1,0), ('down', 1, 0), ('left', 0, -1), ('right', 0, 1)]\n",
    "\n",
    "        # Defining the reward and transition matrices for the environment.\n",
    "        self.reward_matrix, self.transition_matrix = self.get_matrices()\n",
    "\n",
    "    def generate_maze(self):\n",
    "        \"\"\"\n",
    "        Generate a maze using an iterative depth-first search algorithm. \n",
    "        \"\"\"\n",
    "\n",
    "        # Making every other cell in the grid a 1, denoting a cell that will be part of the path but hasn't been visuted by the algorithm yet.\n",
    "            # This means every other cell is currently a wall, and leaves a border of walls around the grid.\n",
    "        self.grid[1::2, 1::2] = 1\n",
    "\n",
    "        # Picking a random unvisited cell and making it the current cell\n",
    "        rand_x = random.randrange(1,self.N,2)\n",
    "        rand_y = random.randrange(1,self.N,2)\n",
    "        current_cell = (rand_x, rand_y) \n",
    "\n",
    "        self.grid[current_cell] = 0 # Adding the current cell to the maze path.\n",
    "\n",
    "        cell_stack = [current_cell] # Creating a stack to store cells to be visited by the algorithm.\n",
    "\n",
    "        # Algorithm runs while there are still cells in the stack to check.\n",
    "        while len(cell_stack) > 0:\n",
    "            # Retrieving most recent cell in stack, removing it from stack, and obtaining its unvisited neighbouring path cells.\n",
    "            current_cell = cell_stack[-1] \n",
    "            cell_stack.pop(-1) \n",
    "            borders = self.get_nearby_cells(current_cell, dist=2)\n",
    "            unvisited_borders = [border for border in borders if self.grid[border] == 1]\n",
    "            # If there are unvisited borders:\n",
    "            if len(unvisited_borders) > 0:\n",
    "                cell_stack.append(current_cell) # Add current cell back to stack.\n",
    "                # Pick bordering cell at random and remove the wall between current cell and that cell, add that cell to stack.\n",
    "                chosen_cell = random.choice(unvisited_borders) \n",
    "                self.grid[(chosen_cell[0] + current_cell[0])//2, (chosen_cell[1] + current_cell[1])//2] = 0\n",
    "                self.grid[chosen_cell] = 0\n",
    "                cell_stack.append(chosen_cell)\n",
    "    \n",
    "    def generate_penalties(self):\n",
    "        \"\"\"\n",
    "        Return a list of the coordinates of every dead end in the maze.\n",
    "        \"\"\"\n",
    "        penalties_pos = []\n",
    "        # Iterating through all path cells to check if they are dead ends.\n",
    "        for i in list(np.argwhere(self.grid == 0)):\n",
    "            path_cell = tuple(i)\n",
    "            # Getting bordering cells, and checking which ones are also path cells.\n",
    "            borders = self.get_nearby_cells(path_cell)\n",
    "            path_borders = [border for border in borders if self.grid[border] == 0]\n",
    "            # If there is only one bordering path cell, then it is dead end.\n",
    "            if len(path_borders) == 1:\n",
    "                penalties_pos.append(path_cell)\n",
    "        return penalties_pos\n",
    "\n",
    "    def get_matrices(self):\n",
    "        \"\"\"\n",
    "        Generate the reward and transition matrices for the environment.\n",
    "\n",
    "        In the reward matrix, a given value represents the reward that the agent would recieve for taking a given action.\n",
    "        Reward matrix indices are [current_state, action_index].\n",
    "\n",
    "        In the transition matrix, a value of 0 means a given action is not allowed, and a 1 means that it is.\n",
    "        Transition matrix indices are [current_state, action_index, next_state].\n",
    " \n",
    "        Action indices - 0: up 1: down 2: left 3: right\n",
    "        \"\"\"\n",
    "        # Creating arrays to hold matrices - reward is by default -1 for a step, and actions are by default disallowed.\n",
    "        reward_matrix = -1 * np.ones((self.N*self.N, 4))\n",
    "        transition_matrix = np.zeros((self.N*self.N, 4, self.N*self.N))\n",
    "\n",
    "        # Iterating through entire grid.\n",
    "        for i in range(self.N):\n",
    "\n",
    "            for j in range(self.N):\n",
    "                current_cell = (i,j)\n",
    "                current_cell_type = self.grid[i, j]\n",
    "                current_state = self.get_state(current_cell)\n",
    "\n",
    "                for action_index, action_tuple in enumerate(self.actions):\n",
    "                    next_cell = (i + action_tuple[1], j + action_tuple[2])\n",
    "\n",
    "                    if self.out_of_bounds(next_cell):\n",
    "                        next_cell_type = current_cell_type\n",
    "                        next_state = current_state\n",
    "                        reward_matrix[current_state, action_index] -= self.N*self.N//4 # Negative reward for going out of bounds.\n",
    "                        transition_matrix[current_state, action_index, next_state] = 1 # Transition from one state to itself is allowed.\n",
    "                    else:\n",
    "                        next_cell_type = self.grid[next_cell]\n",
    "                        next_state = self.get_state(next_cell)\n",
    "\n",
    "                    # Transitions to empty path cells, penalties, and the door are allowed.\n",
    "                    if next_cell_type == 0:\n",
    "                        transition_matrix[current_state, action_index, next_state] = 1 \n",
    "                    elif next_cell_type == 2: \n",
    "                        reward_matrix[current_state, action_index] -= self.N*self.N//4\n",
    "                        transition_matrix[current_state, action_index, next_state] = 0\n",
    "                    elif next_cell_type == 3:\n",
    "                        reward_matrix[current_state, action_index] -= self.N*self.N//2 # Transitioning into a penality incurs a negative reward.\n",
    "                        transition_matrix[current_state, action_index, next_state] = 1\n",
    "                    elif next_cell_type == 4:\n",
    "                        reward_matrix[current_state, action_index] += self.N*self.N # Transitioning into the door gives a large positive reward.\n",
    "                        transition_matrix[current_state, action_index, next_state] = 1\n",
    "\n",
    "        return reward_matrix, transition_matrix \n",
    "\n",
    "    def print_reward_matrices(self):\n",
    "        \"\"\"\n",
    "        Display the reward matrices nicely by action.\n",
    "        \"\"\"    \n",
    "        for action_index, action_tuple in enumerate(self.actions):\n",
    "            print(action_tuple[0])\n",
    "            print(self.reward_matrix[:, action_index].reshape(self.N, self.N))\n",
    "\n",
    "    def get_nearby_cells(self, cell, dist=1):\n",
    "        \"\"\"\n",
    "        For a given cell, get the coordinates of all orthogonal cells that are a given distance away.\n",
    "        dist - distance of desired cells, integer, default 1\n",
    "\n",
    "        Distance of 1 means returning neighbouring cells.\n",
    "        \"\"\"\n",
    "        cell_up = (cell[0]-dist, cell[1])\n",
    "        cell_down = (cell[0]+dist, cell[1])\n",
    "        cell_left = (cell[0], cell[1]-dist)\n",
    "        cell_right = (cell[0], cell[1]+dist)\n",
    "\n",
    "        cell_list = [cell_up, cell_down, cell_left, cell_right]\n",
    "\n",
    "        return [c for c in cell_list if not self.out_of_bounds(c)]\n",
    "\n",
    "    def out_of_bounds(self, cell):\n",
    "        \"\"\"\n",
    "        Check whether given cell coordinates are out of the bounds of the grid.\n",
    "        \"\"\"\n",
    "        if ((cell[0] < 0) or (cell[0] > self.N-1)) or ((cell[1] < 0) or (cell[1] > self.N-1)):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_rand_empty_cells(self, n=1):\n",
    "        \"\"\"\n",
    "        Retreive a random selection of n empty cells from the grid.\n",
    "        n - integer, default 1\n",
    "\n",
    "        Returns a list of tuples specifying the coordinates of the selected cells.\n",
    "        \"\"\"\n",
    "        if n == 1:\n",
    "            zero_cells = np.argwhere(self.grid == 0) # retrieving empty cells\n",
    "            rand_cell = random.choice(zero_cells)\n",
    "            rand_cells = tuple(rand_cell)\n",
    "        else:\n",
    "            zero_cells = np.argwhere(self.grid == 0) \n",
    "            rand_cells = random.choices(zero_cells, k=n)\n",
    "            rand_cells = [tuple(i) for i in rand_cells]\n",
    "\n",
    "        return rand_cells\n",
    "\n",
    "    def get_state(self, cell):\n",
    "        \"\"\"\n",
    "        Return the state number of a given cell.\n",
    "        States are numbered from 0 to (N*N)-1 (N is the size of the grid),\n",
    "        starting in the top left and moving left to right along each row to the bottom right.\n",
    "        \"\"\"\n",
    "        state_grid = np.arange(0, self.N*self.N).reshape(self.N, self.N)\n",
    "        state = state_grid[cell[0], cell[1]]\n",
    "        return state\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"\n",
    "        Display the grid as an image.\n",
    "        \"\"\"\n",
    "        return plt.imshow(self.grid)\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        \"\"\"\n",
    "        One step in the grid.\n",
    "        action_index - the index of the action to be taken, 0: up 1: down 2: left 3: right\n",
    "\n",
    "        Returns:\n",
    "        end - boolean, whether or not the episode has ended\n",
    "        reward - integer, the reward for the taken step\n",
    "        self.state - integer, the new state of the environment after the step\n",
    "        \"\"\"\n",
    "        # Retreive reward value for action from reward matrix.\n",
    "        reward = self.reward_matrix[self.state, action_index] \n",
    "        \n",
    "        # Work out new position and state if action were to be taken.\n",
    "        new_pos = (self.agent_pos[0]+self.actions[action_index][1],\n",
    "                    self.agent_pos[1]+self.actions[action_index][2]) \n",
    "        new_state = self.get_state(new_pos)\n",
    "\n",
    "        self.time_elapsed += 1\n",
    "        end = False\n",
    "\n",
    "        # If transition matrix says action is allowed, do action.\n",
    "        if self.transition_matrix[self.state, action_index, new_state] == 1:\n",
    "            self.grid[self.agent_pos] = 0\n",
    "            self.agent_pos = new_pos\n",
    "            self.grid[self.agent_pos] = 1\n",
    "        \n",
    "        # If agent has reached the door, terminate episode.\n",
    "        if self.agent_pos == self.door_pos:\n",
    "            end = True\n",
    "        \n",
    "        # If number of steps is over the time limit, end episode.\n",
    "        if self.time_elapsed > self.time_limit:\n",
    "            end = True\n",
    "\n",
    "        self.state = self.get_state(self.agent_pos)\n",
    "\n",
    "        return end, reward, self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the grid to its original state, but with the agent in a new random position.\n",
    "        Maze and door remain unchanged.\n",
    "\n",
    "        Returns the new state of the environment.\n",
    "        \"\"\"\n",
    "        for cell in self.penalties_pos:\n",
    "            self.grid[cell] = 3\n",
    "\n",
    "        self.grid[self.door_pos] = 4\n",
    "        \n",
    "        self.grid[self.agent_pos] = 0\n",
    "        self.agent_pos = self.get_rand_empty_cells(1)\n",
    "        self.grid[self.agent_pos] = 1\n",
    "\n",
    "        self.time_elapsed = 0\n",
    "\n",
    "        self.state = self.get_state(self.agent_pos)\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeDQN(Maze):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(N=25)\n",
    "        \n",
    "    def return_start_state(self):\n",
    "        '''Return initial state without having to reset the environment'''\n",
    "        return self.preprocess_state()\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        '''Apply preprocessing to state and reward returned by the step function in the Maze class'''\n",
    "        end, reward, state = super().step(action_index)\n",
    "        # Return state in appropriate format for DQN\n",
    "        input_state = self.preprocess_state()\n",
    "        # Return reward in appropriate format for DQN\n",
    "        reward_tensor = torch.FloatTensor([float(reward)], device=device).unsqueeze(0)\n",
    "        return end, reward_tensor, input_state\n",
    "\n",
    "    def preprocess_state(self):\n",
    "        '''Preprocess state to be used as input for DQN'''\n",
    "        # Calculate coordinates of the agent position relative to the door\n",
    "        relative_coordinates = np.array([self.door_pos[0] - self.agent_pos[0], self.door_pos[1] - self.agent_pos[1]])\n",
    "        \n",
    "        # Pad maze edge with 1s so that taking the surrounding cells of an edge cell does not return an index error\n",
    "        maze_padded = np.ones( (self.N + 2, self.N + 2), dtype = np.int8)\n",
    "        maze_padded[1:self.N+1, 1:self.N+1] = self.grid[:,:]\n",
    "        \n",
    "        # Take surrounding cells using agent's position\n",
    "        surroundings = maze_padded[ self.agent_pos[0] - 1: self.agent_pos[0] + 2,\n",
    "                                     self.agent_pos[1] - 1: self.agent_pos[1] + 2]\n",
    "        surroundings = surroundings.flatten()\n",
    "        \n",
    "        # Preprocess to acquire the state's DQN input format\n",
    "        DQN_input_state = np.concatenate([relative_coordinates, surroundings])\n",
    "        DQN_input_state = torch.FloatTensor(DQN_input_state, device=device).unsqueeze(0)                         \n",
    "        return DQN_input_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining required models and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration for class from Lab 6\n",
    "class ExperienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, N_buffer):\n",
    "        self.size_lim = N_buffer # Define size limit of buffer\n",
    "        self.buffer = deque(maxlen=N_buffer) # Initialise buffer as a list\n",
    "\n",
    "    def store(self, state_tensor, action, reward_tensor, next_state_tensor):\n",
    "        '''Store an experience'''\n",
    "        action_tensor = torch.tensor([action], device=device).unsqueeze(0) # convert action to tensor\n",
    "        experience = Experience(state_tensor, action_tensor, reward_tensor, next_state_tensor) # wrap experience in namedTuple\n",
    "        self.buffer.append(experience) # append to buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def sample(self, BATCH_SIZE):\n",
    "        '''Sample batch from the replay buffer'''\n",
    "        return random.sample(self.buffer, BATCH_SIZE)\n",
    "    \n",
    "    def fill_replay_buffer(self):\n",
    "        '''Fill replay buffer prior to DQN training'''\n",
    "        buffer_filled = False\n",
    "        while not buffer_filled:\n",
    "            maze = MazeDQN() # define a new randomised maze environment each episode\n",
    "            state = maze.return_start_state() # return starting state\n",
    "            end = False # \n",
    "            while not end:\n",
    "                # Obtain action from policy using q values acquired from Q_policy_net\n",
    "                action_index = policy(state, Q_policy_net) \n",
    "                # Take the specified action in an emulator to acquire reward and next state\n",
    "                end, reward, next_state = maze.step(action_index) \n",
    "                if end:\n",
    "                    next_state = None # Set next_state to terminal state label if the goal has been reached\n",
    "                self.store(state, action_index, reward, next_state) # Store the experience in the replay buffer\n",
    "                state = next_state # Update the state\n",
    "                buffer_filled = self.size_lim == len(self) # Check if the buffer is full\n",
    "        print('Buffer Filled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class directly taken from Lab 6 code, already being appropriate for our environment\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, size_hidden, output_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, size_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(size_hidden)\n",
    "        \n",
    "        self.fc2 = nn.Linear(size_hidden, size_hidden)   \n",
    "        self.bn2 = nn.BatchNorm1d(size_hidden)\n",
    "\n",
    "        self.fc3 = nn.Linear(size_hidden, size_hidden)  \n",
    "        self.bn3 = nn.BatchNorm1d(size_hidden)\n",
    "\n",
    "        self.fc4 = nn.Linear(size_hidden, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.bn1(self.fc1(x.float())))\n",
    "        h2 = F.relu(self.bn2(self.fc2(h1)))\n",
    "        h3 = F.relu(self.bn3(self.fc3(h2)))\n",
    "        output = self.fc4(h3.view(h3.size(0), -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration from https://towardsdatascience.com/dueling-deep-q-networks-81ffab672751\n",
    "class DuelingDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, size_hidden, output_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.size_hidden = size_hidden\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.size_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.size_hidden, self.size_hidden),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.state_value_stream = nn.Sequential(\n",
    "            nn.Linear(self.size_hidden, self.size_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.size_hidden, 1))\n",
    "        \n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(self.size_hidden, self.size_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.size_hidden, self.output_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_values = self.hidden_layer(x)\n",
    "        state_values = self.state_value_stream(hidden_values)\n",
    "        advantages = self.advantage_stream(hidden_values)\n",
    "        Q_values = state_values + (advantages - advantages.mean(dim=1).unsqueeze(1))\n",
    "        return Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eGreedyDecayPolicy:\n",
    "\n",
    "    def __init__(self, epsilon = 1, decay = 0.999, min_epsilon = 0.001):\n",
    "        \"\"\"\n",
    "        This class defines an epsilon greedy policy with decay.\n",
    "        epsilon - float, default 1\n",
    "        decay - float, default 0.999\n",
    "\n",
    "        In the case where decay = 1, epsilon will remain constant.\n",
    "        In this case:\n",
    "        If epsilon = 1, the policy will always choose an action randomly.\n",
    "        If epsilon = 0, the policy will always choose greedily. \n",
    "        \"\"\"\n",
    "        self.eps_current = epsilon\n",
    "        self.eps_initial = epsilon\n",
    "        self.min_eps = min_epsilon\n",
    "        self.decay = decay\n",
    "\n",
    "    def __call__(self, state, Q_policy_net):\n",
    "\n",
    "        greedy = random.random() > self.eps_current\n",
    "\n",
    "        if greedy: # if the policy takes the greedy action\n",
    "            with torch.no_grad(): # disable gradient computation\n",
    "                Q_policy_net.eval() # switch to evaluation mode\n",
    "                \n",
    "                # Acquire action index from policy network\n",
    "                action_index = Q_policy_net(state).max(1)[1].view(1, 1).numpy()[0][0]\n",
    "                \n",
    "                Q_policy_net.train() # return to training mode\n",
    "        \n",
    "        else:\n",
    "            action_index = random.randint(0, 3) # return random action\n",
    "        \n",
    "        return action_index\n",
    "    \n",
    "    def update(self):\n",
    "        self.eps_current = self.eps_current*self.decay # apply decay to epsilon\n",
    "        if self.eps_current < self.min_eps:\n",
    "            self.eps_current = self.min_eps # if epsilon has gone below the min epsilon value, set epsilon to the min value\n",
    "\n",
    "    def reset(self):\n",
    "        self.eps_current = self.eps_initial # reset epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from code in Lab 6\n",
    "def optimize_DQN():\n",
    "    \n",
    "    # (state_tensor, action_tensor, reward_tensor, next_state_tensor)\n",
    "\n",
    "    batch = replay_buffer.sample(BATCH_SIZE) # acquire batch\n",
    "    batch = Experience(*zip(*batch)) # make batch experience variables available by variable name\n",
    "    state_batch = torch.cat(batch.state) # collect states\n",
    "    action_batch = torch.cat(batch.action) # collect actions\n",
    "    reward_batch = torch.cat(batch.reward) # collect rewards\n",
    "    \n",
    "    # Collect tensor of boolean values based on next state values: True if the state is not terminal, False if it is\n",
    "    non_final_next_states_mask = torch.tensor(tuple(map(lambda state: state is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    # Collect tensor of next state values for all next states that are not terminal\n",
    "    non_final_next_states = torch.cat([state for state in batch.next_state if state is not None])\n",
    "    \n",
    "    Q_values = Q_policy_net(state_batch).gather(1, action_batch) # compute Q-values through policy network\n",
    "\n",
    "    next_Q_values = torch.zeros(BATCH_SIZE, device=device) # initialise next state Q-values as tensor of zeros\n",
    "    # Compute next state Q-values of non-terminal states using Q network, taking the maximum Q-value for each input\n",
    "    next_Q_values[non_final_next_states_mask] = Q_policy_net(non_final_next_states).max(1)[0].detach()\n",
    "    next_Q_values = next_Q_values.unsqueeze(1)\n",
    "    \n",
    "    # Compute target Q-values\n",
    "    target_Q_values = reward_batch + (next_Q_values * GAMMA) \n",
    "    \n",
    "    # Compute Mean Squared Error Loss\n",
    "    loss = F.mse_loss(Q_values, target_Q_values)\n",
    "    \n",
    "    optimizer.zero_grad() # set optimiser gradients to 0\n",
    "    loss.backward() # calculate gradients through a backwards pass\n",
    "    \n",
    "    # Avoid gradient clipping\n",
    "    for param in Q_policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "    optimizer.step() # perform parameter update based on stored gradients\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from code in Lab 6\n",
    "def train_DQN():\n",
    "    policy.reset() # reset policy hyperparameters\n",
    "    replay_buffer.fill_replay_buffer() # fill buffer\n",
    "    episode_rewards = []\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        maze = MazeDQN() # new maze every episode\n",
    "        state = maze.return_start_state() # acquire first state of the episode\n",
    "        end = False # reset episode termination variable\n",
    "        episode_reward = 0 # reset episode reward\n",
    "\n",
    "        while not end:\n",
    "            # Acquire action index based on q-values from the policy network, using specified policy\n",
    "            action_index = policy(state, Q_policy_net) \n",
    "            \n",
    "            end, reward, next_state = maze.step(action_index) # take the action in an emulator\n",
    "            \n",
    "            if end: # if the agent has reached the exit, set the next state to terminal state label\n",
    "                next_state = None \n",
    "\n",
    "            replay_buffer.store(state, action_index, reward, next_state) # store the transition in memory\n",
    "            state = next_state # update the state\n",
    "            episode_reward += float(reward) # add step reward to episode reward\n",
    "            \n",
    "            # Run a single batch through Double DQN and update the model\n",
    "            loss = optimize_DQN()\n",
    "            \n",
    "        episode_rewards.append(float(episode_reward)) # append episode reward to list of episode rewards\n",
    "        policy.update() # update policy hyperparameters\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "\n",
    "            print('Episode {}: reward : {} epsilon: {} loss: {}'.format(episode, episode_reward, \n",
    "                  policy.eps_current, np.round(loss.detach().numpy(), 3)))   \n",
    "            print('Average of reward for last 100 episodes: {}'.format(sum(episode_rewards[-100:])/100)) \n",
    "    print('Model trained')\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from code in Lab 6\n",
    "def optimize_doubleDQN():\n",
    "    \n",
    "    # (state_tensor, action_tensor, reward_tensor, next_state_tensor)\n",
    "\n",
    "    batch = replay_buffer.sample(BATCH_SIZE) # acquire batch\n",
    "    batch = Experience(*zip(*batch)) # make batch experience variables available by variable name\n",
    "    state_batch = torch.cat(batch.state) # collect states\n",
    "    action_batch = torch.cat(batch.action) # collect actions\n",
    "    reward_batch = torch.cat(batch.reward) # collect rewards\n",
    "    \n",
    "    # Collect tensor of boolean values based on next state values: True if the state is not terminal, False if it is\n",
    "    non_final_next_states_mask = torch.tensor(tuple(map(lambda state: state is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    # Collect tensor of next state values for all next states that are not terminal\n",
    "    non_final_next_states = torch.cat([state for state in batch.next_state if state is not None])\n",
    "    \n",
    "    Q_values = Q_policy_net(state_batch).gather(1, action_batch) # compute q-values through policy network\n",
    "\n",
    "    next_Q_values = torch.zeros(BATCH_SIZE, device=device) # initialise next state q-values as tensor of zeros\n",
    "    # Compute next state q-values of non-terminal states using target Q network, taking the maximum q-value for each input\n",
    "    next_Q_values[non_final_next_states_mask] = Q_target_net(non_final_next_states).max(1)[0].detach()\n",
    "    next_Q_values = next_Q_values.unsqueeze(1)\n",
    "    \n",
    "    # Compute target q-values\n",
    "    target_Q_values = reward_batch + (next_Q_values * GAMMA) \n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = F.mse_loss(Q_values, target_Q_values)\n",
    "    \n",
    "    optimizer.zero_grad() # set optimiser gradients to 0\n",
    "    loss.backward() # calculate gradients through a backwards pass\n",
    "    \n",
    "    # Avoid gradient clipping\n",
    "    for param in Q_policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "    optimizer.step() # perform parameter update based on stored gradients\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from code in Lab 6\n",
    "def train_doubleDQN():\n",
    "    policy.reset() # reset policy hyperparameters\n",
    "    replay_buffer.fill_replay_buffer() # fill buffer\n",
    "    episode_rewards = []\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        maze = MazeDQN() # new maze every episode\n",
    "        state = maze.return_start_state() # acquire first state of the episode\n",
    "        end = False # reset episode termination variable\n",
    "        episode_reward = 0 # reset episode reward\n",
    "\n",
    "        while not end:\n",
    "            # Acquire action index based on q-values from the policy network, using specified policy\n",
    "            action_index = policy(state, Q_policy_net) \n",
    "            \n",
    "            end, reward, next_state = maze.step(action_index) # take the action in an emulator\n",
    "            \n",
    "            if end: # if the agent has reached the exit, set the next state to terminal state label\n",
    "                next_state = None \n",
    "\n",
    "            replay_buffer.store(state, action_index, reward, next_state) # store the transition in memory\n",
    "            state = next_state # update the state\n",
    "            episode_reward += float(reward) # add step reward to episode reward\n",
    "            \n",
    "            # Run a single batch through Double DQN and update the model\n",
    "            loss = optimize_doubleDQN()\n",
    "            \n",
    "        episode_rewards.append(float(episode_reward)) # append episode reward to list of episode rewards\n",
    "        policy.update() # update policy hyperparameters\n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if episode % TARGET_UPDATE_FREQ == 0:\n",
    "            Q_target_net.load_state_dict(Q_policy_net.state_dict())\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "\n",
    "            print('Episode {}: reward : {} epsilon: {} loss: {}'.format(episode, episode_reward, \n",
    "                  policy.eps_current, np.round(loss.detach().numpy(), 3)))   \n",
    "            print('Average of reward for last 100 episodes: {}'.format(sum(episode_rewards[-100:])/100)) \n",
    "    print('Model trained')\n",
    "    return episode_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Q-network and optimiser parameters\n",
    "INPUT_SIZE = 3*3 + 2\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_ACTIONS = 4\n",
    "ALPHA = 0.01\n",
    "\n",
    "# Define policy Q-network and optimiser\n",
    "Q_policy_net = DQN(INPUT_SIZE, HIDDEN_SIZE, NUM_ACTIONS).to(device)\n",
    "optimizer = optim.SGD(Q_policy_net.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DQN training hyperparameters\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.5 # Discount\n",
    "MAZE_SIZE = 25\n",
    "NUM_EPISODES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define policy\n",
    "policy = eGreedyDecayPolicy()\n",
    "# Define buffer\n",
    "replay_buffer = ExperienceReplayBuffer(BUFFER_SIZE)\n",
    "# Train DQN\n",
    "DQN_episode_rewards = train_DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_episode_rewards = np.asarray(DQN_episode_rewards)\n",
    "np.savetxt('vanilla_DQN_results.csv', DQN_episode_rewards, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Q-network and optimiser parameters\n",
    "INPUT_SIZE = 3*3 + 2\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_ACTIONS = 4\n",
    "ALPHA = 0.01\n",
    "\n",
    "# Define policy Q-network and optimiser\n",
    "Q_policy_net = DQN(INPUT_SIZE, HIDDEN_SIZE, NUM_ACTIONS).to(device)\n",
    "optimizer = optim.SGD(Q_policy_net.parameters(), lr=ALPHA)\n",
    "\n",
    "# Define target Q-network\n",
    "Q_target_net = DQN(INPUT_SIZE, HIDDEN_SIZE, NUM_ACTIONS).to(device)\n",
    "# Copy parameter values from policy Q-network\n",
    "Q_target_net.load_state_dict(Q_policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DQN training hyperparameters\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "TARGET_UPDATE_FREQ = 100 # Number of episodes per update of the target DQN parameters\n",
    "GAMMA = 0.5 # Discount\n",
    "MAZE_SIZE = 25\n",
    "NUM_EPISODES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define policy\n",
    "policy = eGreedyDecayPolicy()\n",
    "# Define buffer\n",
    "replay_buffer = ExperienceReplayBuffer(BUFFER_SIZE)\n",
    "# Train DQN\n",
    "DDQN_episode_rewards = train_doubleDQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDQN_episode_rewards = np.asarray(DDQN_episode_rewards)\n",
    "np.savetxt('double_DQN_results.csv', DDQN_episode_rewards, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dueling Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Q-network and optimiser parameters\n",
    "INPUT_SIZE = 3*3 + 2\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_ACTIONS = 4\n",
    "ALPHA = 0.01\n",
    "\n",
    "# Define policy Q-network and optimiser\n",
    "Q_policy_net = DuelingDQN(INPUT_SIZE, HIDDEN_SIZE, NUM_ACTIONS).to(device)\n",
    "optimizer = optim.SGD(Q_policy_net.parameters(), lr=ALPHA)\n",
    "\n",
    "# Define target Q-network\n",
    "Q_target_net = DuelingDQN(INPUT_SIZE, HIDDEN_SIZE, NUM_ACTIONS).to(device)\n",
    "# Copy parameter values from policy Q-network\n",
    "Q_target_net.load_state_dict(Q_policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DQN training hyperparameters\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "TARGET_UPDATE_FREQ = 100 # Number of episodes per update of the target DQN parameters\n",
    "GAMMA = 0.5 # Discount\n",
    "MAZE_SIZE = 25\n",
    "NUM_EPISODES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define policy\n",
    "policy = eGreedyDecayPolicy()\n",
    "# Define buffer\n",
    "replay_buffer = ExperienceReplayBuffer(BUFFER_SIZE)\n",
    "# Train DQN with prioritised experience replay\n",
    "DDDQN_episode_rewards = train_doubleDQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDDQN_episode_rewards = np.asarray(DDDQN_episode_rewards)\n",
    "np.savetxt('DDDQN_results.csv', DDDQN_episode_rewards, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08862e44227d36fa36924244a352dfacc6ee66240882cb9e3f591415d0f56682"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
